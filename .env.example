# ============================================================================# ğŸ”§ Enhanced AI Fashion Extractor Backend Configuration

# ğŸ”§ AI Fashion Extractor - Complete Environment Configuration

# ============================================================================# ============================================================================

# # âš ï¸ SECURITY WARNING

# âš ï¸ SECURITY WARNING:# ============================================================================

# 1. NEVER commit your actual .env file to git# 1. NEVER commit your actual .env file to git

# 2. Keep your API keys secret and rotate them regularly# 2. Keep your API keys secret and rotate them regularly

# 3. Use different keys for development and production# 3. Use different keys for development and production

# 4. Monitor your API usage to detect unauthorized access# 4. Monitor your API usage to detect unauthorized access

##

# To use this template:# To use this template:

# 1. Copy this file: cp .env.example .env# 1. Copy this file: cp .env.example .env

# 2. Replace placeholder values with your actual keys# 2. Replace placeholder values with your actual keys

# 3. Verify .env is in .gitignore (it should be!)# 3. Verify .env is in .gitignore (it should be!)

# ============================================================================# ============================================================================



# ============================================================================# ============================================================================

# ğŸ¤– CORE AI API KEYS (Required)# CORE API KEYS (Required for basic functionality)

# ============================================================================# ============================================================================



# OpenAI GPT-4 Vision (Primary VLM - Most reliable)# OpenAI API Key (Primary VLM)

# Get your key from: https://platform.openai.com/api-keys# Get your key from: https://platform.openai.com/api-keys

# Cost: ~$0.020 per image (2K input + 1.5K output tokens)OPENAI_API_KEY=sk-your-openai-api-key-here

OPENAI_API_KEY=sk-proj-your-openai-api-key-here

# Anthropic Claude API Key (High-quality alternative VLM)

# Anthropic Claude 3.5 Sonnet (High-quality alternative)# Get your key from: https://console.anthropic.com/

# Get your key from: https://console.anthropic.com/ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# Cost: ~$0.0285 per image (best for complex fashion attributes)

ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here# Google Gemini API Key (Fast and cost-effective VLM)

# Get your key from: https://makersuite.google.com/app/apikey

# Google Gemini 1.5 Flash (Fast and cost-effective)GOOGLE_API_KEY=your-google-api-key-here

# Get your key from: https://makersuite.google.com/app/apikey

# Cost: ~$0.0006 per image (50x cheaper than OpenAI, great for large batches)# HuggingFace API Token (Fashion-CLIP + LLaVA)

GOOGLE_API_KEY=your-google-api-key-here# Get your token from: https://huggingface.co/settings/tokens

HUGGINGFACE_API_KEY=hf_your-huggingface-token-here

# HuggingFace API Token (Fallback models: Fashion-CLIP + LLaVA)

# Get your token from: https://huggingface.co/settings/tokens# ============================================================================

# Cost: Free tier available, then pay-as-you-go# VLM FUSION MODE (Multi-Model AI Combination)

HUGGINGFACE_API_KEY=hf_your-huggingface-token-here# ============================================================================



# ============================================================================# Fusion Mode: How to combine multiple AI models

# ğŸ”€ VLM FUSION MODE (Multi-Model AI Configuration)# Options:

# ============================================================================#   - 'default' (fallback mode - use one model at a time, cheaper)

# Controls how multiple AI models work together#   - 'balanced' (2 models, confidence-weighted, recommended for production)

##   - 'high-quality' (3 models, voting, best accuracy but expensive)

# Available Modes:#   - 'cost-optimized' (cheapest models, good for large batches)

# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”VLM_FUSION_MODE=default

# â”‚ Mode            â”‚ Models Used â”‚ Cost/Image   â”‚ Accuracy   â”‚ Speed       â”‚

# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤# ============================================================================

# â”‚ default         â”‚ 1 (OpenAI)  â”‚ $0.020       â”‚ 85-90%     â”‚ Fast (3s)   â”‚# SERVER CONFIGURATION

# â”‚ balanced        â”‚ 2 (Claude+  â”‚ $0.029       â”‚ 90-95%     â”‚ Medium (5s) â”‚# ============================================================================

# â”‚                 â”‚    Gemini)  â”‚              â”‚            â”‚             â”‚

# â”‚ high-quality    â”‚ 3 (All)     â”‚ $0.049       â”‚ 95-98%     â”‚ Slow (8s)   â”‚# Server port

# â”‚ cost-optimized  â”‚ 1 (Gemini)  â”‚ $0.0006      â”‚ 80-85%     â”‚ Fast (2s)   â”‚PORT=5000

# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

## Environment mode

# Recommendations:NODE_ENV=development

# - Development: default (cheapest, fast testing)

# - Production (Small batches <5K): balanced â­ RECOMMENDED# Frontend URL for CORS

# - Production (Quality critical): high-qualityFRONTEND_URL=http://localhost:5173

# - Production (Large batches >40K): cost-optimized

## Maximum file upload size (15MB for high-quality images)

# HOW IT WORKS:MAX_FILE_SIZE=15728640

# - 'default': Fallback mode - uses ONE model, tries next if fails

# - 'balanced': Fusion mode - uses TWO models, combines results intelligently# ============================================================================

# - 'high-quality': Fusion mode - uses THREE models, voting for best accuracy# REDIS CACHE CONFIGURATION (Optional but Highly Recommended)

# - 'cost-optimized': Single model - fastest/cheapest for large volumes# ============================================================================

#

VLM_FUSION_MODE=balanced
#Enable Redis caching for massive performance improvements

# Set to 'true' to enable, 'false' to disable

# ============================================================================ENABLE_REDIS=false

# ğŸŒ SERVER CONFIGURATION

# ============================================================================# Redis connection settings

REDIS_HOST=localhost

# Server port (default: 5000)REDIS_PORT=6379

PORT=5000

# Redis password (if authentication is enabled)

# Environment mode# REDIS_PASSWORD=your_secure_redis_password

# Options: development | production | staging

NODE_ENV=development
# Alternative: Use Redis connection URL (for cloud providers like Redis Cloud, Upstash)

# REDIS_URL=redis://username:password@hostname:port

# Frontend URL for CORS (your React app URL)

# Development: http://localhost:5173# Benefits of Redis Caching:

# Production: https://your-vercel-app.vercel.app# - 95%+ faster for repeat requests (50ms vs 5s)

FRONTEND_URL=http://localhost:5173
# - 60-75% reduction in API costs

# - Better scalability for high traffic

# Maximum file upload size in bytes# - Automatic cache expiration (1 hour default)

# 15728640 bytes = 15MB (recommended for high-quality images)#

MAX_FILE_SIZE=15728640
# To set up Redis locally:

# 1. Install Redis: https://redis.io/download

# ============================================================================# 2. Start Redis server: redis-server

# ğŸ” AUTHENTICATION & SECURITY# 3. Set ENABLE_REDIS=true above

# ============================================================================# 4. Restart your backend server



# JWT Secret for authentication tokens# ============================================================================

# âš ï¸ CHANGE THIS IN PRODUCTION! Use a strong random string (32+ characters)# VLM (Vision Language Model) CONFIGURATION

# Generate with: openssl rand -base64 32# ============================================================================

JWT_SECRET=your-super-secret-jwt-key-change-in-production-2025

# Primary VLM Provider

# ============================================================================# Options: fashion-clip | ollama-llava | huggingface-llava | openai-gpt4v

# ğŸš€ REDIS CACHE CONFIGURATION (Highly Recommended for Production)VLM_PRIMARY_PROVIDER=fashion-clip

# ============================================================================

# Benefits of Redis Caching:# Confidence threshold for fallback triggers (0-100)

# âœ… 95%+ faster for repeat requests (50ms vs 5s response time)VLM_CONFIDENCE_THRESHOLD=70

# âœ… 60-75% reduction in API costs (cached results = no API calls)

# âœ… Better scalability for high traffic# Maximum processing timeout (milliseconds)

# âœ… Automatic cache expiration (1 hour default)VLM_TIMEOUT_MS=60000

# âœ… Reduces load on AI providers

## Enable discovery mode by default

# Cost Savings Example (1000 images, 30% cache hits):VLM_ENABLE_DISCOVERY=true

# - Without cache: 1000 images Ã— $0.020 = $20.00

# - With cache:    700 images Ã— $0.020 = $14.00 (30% saved!)# Enable fashion-specific optimizations

# ============================================================================VLM_FASHION_OPTIMIZATION=true



# Enable Redis caching (true = enabled, false = disabled)# ============================================================================

ENABLE_REDIS=true# LOCAL VLM CONFIGURATION (Privacy & Cost Optimization)

# ============================================================================

# Redis connection settings

# For local development (install Redis locally):# Ollama server URL (for local LLaVA processing)

REDIS_HOST=localhost# Install: curl -fsSL https://ollama.ai/install.sh | sh

REDIS_PORT=6379# Models: ollama pull llava:7b && ollama pull moondream

REDIS_PASSWORD=OLLAMA_BASE_URL=http://localhost:11434



# For cloud Redis (Redis Cloud, Upstash, AWS ElastiCache):# Default Ollama model

# REDIS_HOST=your-redis-host.cloud.redislabs.comOLLAMA_MODEL=llava:7b

# REDIS_PORT=12345

# REDIS_PASSWORD=your-redis-password-here# Enable local processing priority (reduces API costs)

OLLAMA_PRIORITY=true

# Alternative: Full Redis connection URL (preferred for cloud providers)

# Format: redis://[username]:[password]@[host]:[port]# ============================================================================

# REDIS_URL=redis://default:password@hostname:port# HUGGINGFACE VLM CONFIGURATION

# ============================================================================

# Redis Cloud Example:

# REDIS_URL=redis://default:oc7V5c6FasqCVf71FT3h92bPj9mNnj9r@redis-11543.c80.us-east-1-2.ec2.redns.redis-cloud.com:11543# HuggingFace Inference API base URL

HUGGINGFACE_BASE_URL=https://api-inference.huggingface.co

# To set up Redis locally:

# 1. Install Redis: https://redis.io/download# Default HuggingFace LLaVA model

# 2. Start Redis server: redis-serverHUGGINGFACE_MODEL=llava-hf/llava-1.5-13b-hf

# 3. Set ENABLE_REDIS=true above

# 4. Leave REDIS_PASSWORD empty for local# Fashion-CLIP model for specialized classification

# 5. Restart your backend serverFASHION_CLIP_MODEL=openai/clip-vit-base-patch32



# ============================================================================# ============================================================================

# ğŸ—„ï¸ DATABASE CONFIGURATION (PostgreSQL)# OPENAI VLM CONFIGURATION

# ============================================================================# ============================================================================



# Supabase API Configuration# OpenAI API base URL

# Get from: https://app.supabase.com/project/YOUR_PROJECT/settings/apiOPENAI_BASE_URL=https://api.openai.com/v1

SUPABASE_URL=https://your-project.supabase.co

SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.your-anon-key-here
# Default OpenAI Vision model

SUPABASE_SERVICE_ROLE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.your-service-role-key-hereOPENAI_VISION_MODEL=gpt-4o


# PostgreSQL Database URL (Neon, Supabase, or other provider)# Image detail level for OpenAI Vision API

# Format: postgresql://[user]:[password]@[host]/[database]?sslmode=requireOPENAI_IMAGE_DETAIL=high

# Get from: Neon Dashboard â†’ Connection String

DATABASE_URL=postgresql://user:password@host.neon.tech/database?sslmode=require
# ============================================================================

# PERFORMANCE OPTIMIZATION

# ============================================================================# ============================================================================

# ğŸ“Š SENTRY MONITORING (Error Tracking & Performance)

# ============================================================================# Enable response caching (reduces duplicate API calls)

# Get DSN from: https://sentry.io/settings/projects/ENABLE_RESPONSE_CACHE=true

#

# Benefits:# Cache expiration time (minutes)

# âœ… Real-time error tracking and alertsCACHE_EXPIRATION_MINUTES=60

# âœ… Performance monitoring (API response times)

# âœ… User session replay for debugging# Maximum concurrent VLM requests

# âœ… Release tracking and source mapsMAX_CONCURRENT_REQUESTS=5

# âœ… Production issue detection before users report

# ============================================================================# Enable request queuing for high load

ENABLE_REQUEST_QUEUE=true

# Enable Sentry monitoring (true for production, false for development)

SENTRY_ENABLED=true# ============================================================================

# FASHION-SPECIFIC CONFIGURATION

# Your Sentry DSN (Data Source Name) - Backend project# ============================================================================

# Get from: Sentry â†’ Settings â†’ Projects â†’ [Your Project] â†’ Client Keys (DSN)

SENTRY_DSN=https://your-sentry-dsn@o123456.ingest.sentry.io/123456# Enable brand detection

ENABLE_BRAND_DETECTION=true

# Application version (for release tracking)

# Update this with each deployment to track which version has issues# Enable fabric analysis

APP_VERSION=1.0.0ENABLE_FABRIC_ANALYSIS=true



# Enable Sentry debug mode (shows logs in console, useful for testing)# Enable construction details extraction

# Set to 'true' in development, 'false' in productionENABLE_CONSTRUCTION_DETAILS=true

SENTRY_DEBUG=false

# Enable care label reading

# Sentry Auth Token (optional, for uploading source maps)ENABLE_CARE_LABEL_READING=true

# Get from: https://sentry.io/settings/account/api/auth-tokens/

# Only needed if you want to upload source maps for better error tracking# Fashion database path (optional)

# SENTRY_AUTH_TOKEN=sntrys_your_auth_token_hereFASHION_DATABASE_PATH=./data/fashion_embeddings.db



# ============================================================================# ============================================================================

# ğŸ–¥ï¸ OPTIONAL: LOCAL AI MODELS (Ollama)# LOGGING & MONITORING

# ============================================================================# ============================================================================

# Ollama allows running AI models locally without API costs

# Benefits:# Enable verbose VLM logging

# âœ… No API costs (free inference)VLM_VERBOSE_LOGGING=true

# âœ… Full data privacy (no data sent to cloud)

# âœ… Works offline# Log level (error | warn | info | debug)

# âŒ Requires local GPU (NVIDIA recommended)LOG_LEVEL=info

# âŒ Slower than cloud APIs

## Enable performance metrics collection

# Setup:ENABLE_PERFORMANCE_METRICS=true

# 1. Install Ollama: https://ollama.ai/download

# 2. Pull LLaVA model: ollama pull llava:7b# Enable provider health monitoring

# 3. Start Ollama: ollama serve (runs on port 11434)ENABLE_HEALTH_MONITORING=true

# 4. Uncomment line below

# ============================================================================# Health check interval (seconds)

HEALTH_CHECK_INTERVAL=300

# Ollama local server URL

# OLLAMA_BASE_URL=http://localhost:11434# ============================================================================

# SECURITY CONFIGURATION

# ============================================================================# ============================================================================

# ğŸ“ˆ MONITORING & PERFORMANCE

# ============================================================================# Enable rate limiting

ENABLE_RATE_LIMITING=true

# Enable performance metrics collection (API response times, token usage)

ENABLE_PERFORMANCE_METRICS=true# Rate limit: requests per minute per IP

RATE_LIMIT_RPM=100

# Enable provider health monitoring (checks if AI models are available)

ENABLE_HEALTH_MONITORING=true# Enable input validation

ENABLE_INPUT_VALIDATION=true

# Health check interval in seconds (how often to check AI model health)

HEALTH_CHECK_INTERVAL=300# Maximum image dimensions (pixels)

MAX_IMAGE_WIDTH=4096

# ============================================================================MAX_IMAGE_HEIGHT=4096

# ğŸ”’ SECURITY CONFIGURATION

# ============================================================================# Allowed image formats

ALLOWED_IMAGE_FORMATS=jpeg,jpg,png,webp,tiff

# Enable rate limiting (prevents abuse and DDoS attacks)

ENABLE_RATE_LIMITING=true# ============================================================================

# EXPERIMENTAL FEATURES (Beta)

# Rate limit: requests per minute per IP address# ============================================================================

RATE_LIMIT_RPM=100

# Enable multi-language support

# Enable input validation (validates image format, size, etc.)ENABLE_MULTILANG=false

ENABLE_INPUT_VALIDATION=true

# Enable 3D garment analysis (experimental)

# Maximum image dimensions (pixels)ENABLE_3D_ANALYSIS=false

MAX_IMAGE_WIDTH=4096

MAX_IMAGE_HEIGHT=4096# Enable seasonal trend detection

ENABLE_TREND_DETECTION=false

# Allowed image formats (comma-separated)

ALLOWED_IMAGE_FORMATS=jpeg,jpg,png,webp,tiff# Enable sustainable fashion analysis

ENABLE_SUSTAINABILITY_ANALYSIS=false

# ============================================================================

# ğŸ› ï¸ DEVELOPMENT & DEBUG (Development only, disable in production)# ============================================================================

# ============================================================================# DEVELOPMENT & DEBUG

# ============================================================================

# Enable development mode features (verbose logging, etc.)

DEVELOPMENT_MODE=true# Enable development mode features

DEVELOPMENT_MODE=true

# Enable API endpoint debugging (logs all API requests/responses)

DEBUG_API_ENDPOINTS=false# Enable API endpoint debugging

DEBUG_API_ENDPOINTS=true

# Enable VLM pipeline debugging (logs AI model interactions)

DEBUG_VLM_PIPELINE=false# Enable VLM pipeline debugging

DEBUG_VLM_PIPELINE=true

# Save debug images to disk (useful for troubleshooting)

SAVE_DEBUG_IMAGES=false# Save debug images to disk

SAVE_DEBUG_IMAGES=false

# Debug images directory (where to save debug images)

DEBUG_IMAGES_DIR=./debug/images# Debug images directory

DEBUG_IMAGES_DIR=./debug/images

# ============================================================================

# ğŸ§ª EXPERIMENTAL FEATURES (Beta - Not Production Ready)# ============================================================================

# ============================================================================# INSTRUCTIONS FOR SETUP

# ============================================================================

# Enable multi-language support (detect and extract in multiple languages)

ENABLE_MULTILANG=false# 1. MINIMUM SETUP (OpenAI only):

#    - Set OPENAI_API_KEY

# Enable 3D garment analysis (experimental, requires additional models)#    - Start with: npm run dev

ENABLE_3D_ANALYSIS=false

# 2. ENHANCED SETUP (Multiple VLMs):

# Enable seasonal trend detection (analyze fashion trends)#    - Set OPENAI_API_KEY and HUGGINGFACE_API_KEY

ENABLE_TREND_DETECTION=false#    - Install Ollama: curl -fsSL https://ollama.ai/install.sh | sh

#    - Pull models: ollama pull llava:7b

# Enable sustainable fashion analysis (eco-friendly material detection)#    - Start with: npm run dev

ENABLE_SUSTAINABILITY_ANALYSIS=false

# 3. FULL PRODUCTION SETUP:

# ============================================================================#    - Configure all API keys

# ğŸ“‹ SETUP INSTRUCTIONS#    - Set up local Ollama server

# ============================================================================#    - Configure caching and monitoring

##    - Set NODE_ENV=production

# MINIMUM SETUP (OpenAI only - $0.020/image):#    - Use process manager (PM2)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# 1. Set OPENAI_API_KEY# ============================================================================

# 2. Set DATABASE_URL (PostgreSQL)# PROVIDER SELECTION STRATEGY

# 3. Run: npm install# ============================================================================

# 4. Run: npm run dev

# 5. Test with sample image# The system automatically selects the best provider based on:

## 1. Provider availability (health checks)

# RECOMMENDED SETUP (Balanced fusion - $0.029/image):# 2. Request type (fashion-focused vs general)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€# 3. Required confidence level

# 1. Set OPENAI_API_KEY# 4. Cost optimization preferences

# 2. Set ANTHROPIC_API_KEY# 5. Processing time requirements

# 3. Set GOOGLE_API_KEY

# 4. Set VLM_FUSION_MODE=balanced# Fallback chain (when primary fails):

# 5. Set DATABASE_URL# fashion-clip -> ollama-llava -> huggingface-llava -> openai-gpt4v
# 6. Set up Redis (optional but recommended)
# 7. Run: npm install
# 8. Run: npm run dev
#
# PRODUCTION SETUP (Full features):
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Configure all API keys (OpenAI, Claude, Google, HuggingFace)
# 2. Set VLM_FUSION_MODE (balanced for most use cases)
# 3. Set up Redis cache (ENABLE_REDIS=true)
# 4. Configure PostgreSQL (DATABASE_URL)
# 5. Enable Sentry monitoring (SENTRY_ENABLED=true)
# 6. Set NODE_ENV=production
# 7. Change JWT_SECRET to strong random value
# 8. Set FRONTEND_URL to your production domain
# 9. Deploy to Render/Railway/AWS
# 10. Monitor costs and performance
#
# ============================================================================
# ğŸ’° COST ESTIMATION (Per 1000 Images)
# ============================================================================
#
# Mode: default (OpenAI only)
#   Cost: $20.00
#   Accuracy: 85-90%
#   Speed: ~3s per image
#   Best for: Development, testing, small batches
#
# Mode: balanced (Claude + Gemini) â­ RECOMMENDED
#   Cost: $29.10
#   Accuracy: 90-95%
#   Speed: ~5s per image
#   Best for: Production, quality matters, medium batches
#
# Mode: high-quality (OpenAI + Claude + Gemini)
#   Cost: $49.10
#   Accuracy: 95-98%
#   Speed: ~8s per image
#   Best for: High-value products, fashion catalogs, small batches
#
# Mode: cost-optimized (Gemini only)
#   Cost: $0.60
#   Accuracy: 80-85%
#   Speed: ~2s per image
#   Best for: Large batches (>40K images), budget-constrained projects
#
# With Redis Cache (30% hit rate):
#   default: $20.00 â†’ $14.00 (30% savings)
#   balanced: $29.10 â†’ $20.40 (30% savings)
#   high-quality: $49.10 â†’ $34.40 (30% savings)
#
# ============================================================================
# ğŸ¯ PROVIDER SELECTION STRATEGY
# ============================================================================
#
# The system automatically selects the best provider based on:
# 1. Provider availability (health checks)
# 2. Request type (fashion-focused vs general)
# 3. Required confidence level
# 4. Cost optimization preferences
# 5. Processing time requirements
#
# Fallback chain (when primary fails):
# openai-gpt4v â†’ claude-sonnet â†’ google-gemini â†’ huggingface-llava
#
# Fusion mode (when enabled):
# Run multiple models in parallel â†’ Combine results using:
#   - Voting (majority wins, best for categorical attributes)
#   - Confidence-weighted (smart weighting, best for mixed attributes)
#   - Best-only (use best model, fill gaps from others)
#
# ============================================================================
# ğŸ“š DOCUMENTATION
# ============================================================================
#
# Full documentation available:
# - SYSTEM_AUDIT_REPORT.md (15 pages) - Architecture and optimization
# - MULTI_MODEL_FUSION_GUIDE.md (10 pages) - How fusion works
# - DEPLOYMENT_READINESS.md (12 pages) - Deployment checklist
# - COST_QUOTATION_40K_IMAGES.md (20 pages) - Detailed pricing
# - ERROR_CHECK_STATUS.md - Current system status
#
# Support:
# - GitHub Issues: https://github.com/your-repo/issues
# - Email: support@your-domain.com
#
# ============================================================================
