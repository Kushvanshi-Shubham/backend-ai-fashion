# ðŸ”§ Enhanced AI Fashion Extractor Backend Configuration

# ============================================================================
# CORE API KEYS (Required for basic functionality)
# ============================================================================

# OpenAI API Key (Primary/Fallback VLM)
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-openai-api-key-here

# HuggingFace API Token (Fashion-CLIP + LLaVA)
# Get your token from: https://huggingface.co/settings/tokens
HUGGINGFACE_API_KEY=hf_your-huggingface-token-here

# ============================================================================
# SERVER CONFIGURATION
# ============================================================================

# Server port
PORT=5000

# Environment mode
NODE_ENV=development

# Frontend URL for CORS
FRONTEND_URL=http://localhost:5173

# Maximum file upload size (15MB for high-quality images)
MAX_FILE_SIZE=15728640

# ============================================================================
# VLM (Vision Language Model) CONFIGURATION
# ============================================================================

# Primary VLM Provider
# Options: fashion-clip | ollama-llava | huggingface-llava | openai-gpt4v
VLM_PRIMARY_PROVIDER=fashion-clip

# Confidence threshold for fallback triggers (0-100)
VLM_CONFIDENCE_THRESHOLD=70

# Maximum processing timeout (milliseconds)
VLM_TIMEOUT_MS=60000

# Enable discovery mode by default
VLM_ENABLE_DISCOVERY=true

# Enable fashion-specific optimizations
VLM_FASHION_OPTIMIZATION=true

# ============================================================================
# LOCAL VLM CONFIGURATION (Privacy & Cost Optimization)
# ============================================================================

# Ollama server URL (for local LLaVA processing)
# Install: curl -fsSL https://ollama.ai/install.sh | sh
# Models: ollama pull llava:7b && ollama pull moondream
OLLAMA_BASE_URL=http://localhost:11434

# Default Ollama model
OLLAMA_MODEL=llava:7b

# Enable local processing priority (reduces API costs)
OLLAMA_PRIORITY=true

# ============================================================================
# HUGGINGFACE VLM CONFIGURATION
# ============================================================================

# HuggingFace Inference API base URL
HUGGINGFACE_BASE_URL=https://api-inference.huggingface.co

# Default HuggingFace LLaVA model
HUGGINGFACE_MODEL=llava-hf/llava-1.5-13b-hf

# Fashion-CLIP model for specialized classification
FASHION_CLIP_MODEL=openai/clip-vit-base-patch32

# ============================================================================
# OPENAI VLM CONFIGURATION
# ============================================================================

# OpenAI API base URL
OPENAI_BASE_URL=https://api.openai.com/v1

# Default OpenAI Vision model
OPENAI_VISION_MODEL=gpt-4o

# Image detail level for OpenAI Vision API
OPENAI_IMAGE_DETAIL=high

# ============================================================================
# PERFORMANCE OPTIMIZATION
# ============================================================================

# Enable response caching (reduces duplicate API calls)
ENABLE_RESPONSE_CACHE=true

# Cache expiration time (minutes)
CACHE_EXPIRATION_MINUTES=60

# Maximum concurrent VLM requests
MAX_CONCURRENT_REQUESTS=5

# Enable request queuing for high load
ENABLE_REQUEST_QUEUE=true

# ============================================================================
# FASHION-SPECIFIC CONFIGURATION
# ============================================================================

# Enable brand detection
ENABLE_BRAND_DETECTION=true

# Enable fabric analysis
ENABLE_FABRIC_ANALYSIS=true

# Enable construction details extraction
ENABLE_CONSTRUCTION_DETAILS=true

# Enable care label reading
ENABLE_CARE_LABEL_READING=true

# Fashion database path (optional)
FASHION_DATABASE_PATH=./data/fashion_embeddings.db

# ============================================================================
# LOGGING & MONITORING
# ============================================================================

# Enable verbose VLM logging
VLM_VERBOSE_LOGGING=true

# Log level (error | warn | info | debug)
LOG_LEVEL=info

# Enable performance metrics collection
ENABLE_PERFORMANCE_METRICS=true

# Enable provider health monitoring
ENABLE_HEALTH_MONITORING=true

# Health check interval (seconds)
HEALTH_CHECK_INTERVAL=300

# ============================================================================
# SECURITY CONFIGURATION
# ============================================================================

# Enable rate limiting
ENABLE_RATE_LIMITING=true

# Rate limit: requests per minute per IP
RATE_LIMIT_RPM=100

# Enable input validation
ENABLE_INPUT_VALIDATION=true

# Maximum image dimensions (pixels)
MAX_IMAGE_WIDTH=4096
MAX_IMAGE_HEIGHT=4096

# Allowed image formats
ALLOWED_IMAGE_FORMATS=jpeg,jpg,png,webp,tiff

# ============================================================================
# EXPERIMENTAL FEATURES (Beta)
# ============================================================================

# Enable multi-language support
ENABLE_MULTILANG=false

# Enable 3D garment analysis (experimental)
ENABLE_3D_ANALYSIS=false

# Enable seasonal trend detection
ENABLE_TREND_DETECTION=false

# Enable sustainable fashion analysis
ENABLE_SUSTAINABILITY_ANALYSIS=false

# ============================================================================
# DEVELOPMENT & DEBUG
# ============================================================================

# Enable development mode features
DEVELOPMENT_MODE=true

# Enable API endpoint debugging
DEBUG_API_ENDPOINTS=true

# Enable VLM pipeline debugging
DEBUG_VLM_PIPELINE=true

# Save debug images to disk
SAVE_DEBUG_IMAGES=false

# Debug images directory
DEBUG_IMAGES_DIR=./debug/images

# ============================================================================
# INSTRUCTIONS FOR SETUP
# ============================================================================

# 1. MINIMUM SETUP (OpenAI only):
#    - Set OPENAI_API_KEY
#    - Start with: npm run dev

# 2. ENHANCED SETUP (Multiple VLMs):
#    - Set OPENAI_API_KEY and HUGGINGFACE_API_KEY
#    - Install Ollama: curl -fsSL https://ollama.ai/install.sh | sh
#    - Pull models: ollama pull llava:7b
#    - Start with: npm run dev

# 3. FULL PRODUCTION SETUP:
#    - Configure all API keys
#    - Set up local Ollama server
#    - Configure caching and monitoring
#    - Set NODE_ENV=production
#    - Use process manager (PM2)

# ============================================================================
# PROVIDER SELECTION STRATEGY
# ============================================================================

# The system automatically selects the best provider based on:
# 1. Provider availability (health checks)
# 2. Request type (fashion-focused vs general)
# 3. Required confidence level
# 4. Cost optimization preferences
# 5. Processing time requirements

# Fallback chain (when primary fails):
# fashion-clip -> ollama-llava -> huggingface-llava -> openai-gpt4v